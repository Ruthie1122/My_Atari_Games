{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmSGG4orUTX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d18e4bf-76fc-4791-b772-cc886a349a76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.10/dist-packages (2.3.2)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.4.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.1.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.17.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.8.0)\n",
            "Collecting shimmy~=1.3.0 (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra])\n",
            "  Using cached Shimmy-1.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Requirement already satisfied: autorom~=0.6.1 in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.32.3)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra]) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.64.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (71.0.4)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.15.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.6.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0; extra == \"extra\"->stable-baselines3[extra]) (6.4.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Using cached Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: shimmy\n",
            "  Attempting uninstall: shimmy\n",
            "    Found existing installation: Shimmy 0.2.1\n",
            "    Uninstalling Shimmy-0.2.1:\n",
            "      Successfully uninstalled Shimmy-0.2.1\n",
            "Successfully installed shimmy-1.3.0\n",
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n",
            "  Using cached Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.4)\n",
            "Using cached Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: shimmy\n",
            "  Attempting uninstall: shimmy\n",
            "    Found existing installation: Shimmy 1.3.0\n",
            "    Uninstalling Shimmy-1.3.0:\n",
            "      Successfully uninstalled Shimmy-1.3.0\n",
            "Successfully installed shimmy-0.2.1\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install gymnasium[atari]\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_axp7tInITj_"
      },
      "source": [
        "# importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bu3nH1OnDm88"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
        "from stable_baselines3.dqn.policies import CnnPolicy\n",
        "from gymnasium.wrappers import FrameStack, ResizeObservation\n",
        "from gymnasium.utils.save_video import save_video\n",
        "from PIL import Image\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klfgf7Hu9DNw"
      },
      "source": [
        "# CartPole Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfSErqIyqlVU"
      },
      "outputs": [],
      "source": [
        "class CartPoleDQNAgent:\n",
        "    def __init__(self, name=None, env_name=None, eval_freq=20000, buffer_size=1000):\n",
        "        self.name = name\n",
        "        self.env_name = env_name\n",
        "        self.policy = \"MlpPolicy\"\n",
        "        self.eval_freq = eval_freq\n",
        "        self.buffer_size = buffer_size\n",
        "        self.log_path = os.path.join('Training/DQN_' + self.name + '_Log')\n",
        "        self.save_path = os.path.join('Saved_Models/DQN_' + self.name + '_Model')\n",
        "        self.env = self.make_environment()\n",
        "        self.model = self._build_dqn()\n",
        "\n",
        "    def make_environment(self):\n",
        "        env = gym.make(self.env_name, render_mode=\"rgb_array\")\n",
        "        env = DummyVecEnv([lambda: env])\n",
        "        return env\n",
        "\n",
        "    def _build_dqn(self):\n",
        "        model = DQN(policy=self.policy, env=self.env, verbose=0, tensorboard_log=self.log_path, buffer_size=self.buffer_size)\n",
        "        return model\n",
        "\n",
        "    def _play_one_episode(self):\n",
        "        obs = self.env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "\n",
        "        while not done:\n",
        "            action = self.env.action_space.sample()\n",
        "            obs, reward, done, _ = self.env.step([action])\n",
        "            score += reward\n",
        "\n",
        "        return score\n",
        "\n",
        "    def play_episodes(self, num_episodes=10, play_type=\"random\"):\n",
        "        if play_type == \"random\":\n",
        "            print(f\"Playing the {self.name} game randomly for {num_episodes} episodes\")\n",
        "            scores = [self._play_one_episode() for _ in range(num_episodes)]\n",
        "            for episode, score in enumerate(scores, 1):\n",
        "                print(f\"Episode {episode}: {score}\")\n",
        "\n",
        "        if play_type == \"predict\":\n",
        "            episode_rewards = []\n",
        "            frames = []\n",
        "\n",
        "            for episode in range(num_episodes):\n",
        "                obs = self.env.reset()\n",
        "                done = False\n",
        "                score = 0\n",
        "\n",
        "                while not done:\n",
        "                    action, _ = self.model.predict(obs)\n",
        "                    obs, reward, done, *info = self.env.step(action)\n",
        "                    score += reward\n",
        "                    frame = Image.fromarray(self.env.render())\n",
        "                    frame = np.array(frame)\n",
        "                    frames.append(frame)\n",
        "\n",
        "                episode_rewards.append(score)\n",
        "\n",
        "                print(f\"Episode {episode+1}: {score}\")\n",
        "\n",
        "            video_path = os.path.join(self.save_path, self.name + \"_Agent_play\")\n",
        "\n",
        "            save_video(frames, video_path, fps=30, name_prefix=f\"{self.name}-agent-play\")\n",
        "\n",
        "    def train(self, time_steps=None, stop_value=None):\n",
        "        stop_callback = StopTrainingOnRewardThreshold(reward_threshold=stop_value, verbose=0)\n",
        "        eval_callback = EvalCallback(self.env, callback_on_new_best=stop_callback, eval_freq=self.eval_freq, best_model_save_path=self.save_path)\n",
        "        self.model.learn(total_timesteps=time_steps, callback=eval_callback)\n",
        "\n",
        "    def evaluate_policy(self, episodes=None):\n",
        "        mean_reward, reward_std = evaluate_policy(self.model, self.env, n_eval_episodes=episodes)\n",
        "        print(f\"Mean reward over {episodes} episodes is {mean_reward} with a standard deviation of {reward_std}\")\n",
        "\n",
        "    def close_env(self):\n",
        "        self.env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2SNSMa3qlYH"
      },
      "outputs": [],
      "source": [
        "#create the agent and create the environment\n",
        "CartPole_agent = CartPoleDQNAgent(name=\"CartPole\", env_name=\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEjjHwaZAQqZ",
        "outputId": "1f4c04ec-f4fd-44b7-9a4b-d2bf13dac3f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Playing the CartPole game randomly for 30 episodes\n",
            "Episode 1: [38.]\n",
            "Episode 2: [24.]\n",
            "Episode 3: [48.]\n",
            "Episode 4: [14.]\n",
            "Episode 5: [15.]\n",
            "Episode 6: [16.]\n",
            "Episode 7: [29.]\n",
            "Episode 8: [26.]\n",
            "Episode 9: [14.]\n",
            "Episode 10: [13.]\n",
            "Episode 11: [17.]\n",
            "Episode 12: [11.]\n",
            "Episode 13: [19.]\n",
            "Episode 14: [28.]\n",
            "Episode 15: [19.]\n",
            "Episode 16: [48.]\n",
            "Episode 17: [33.]\n",
            "Episode 18: [15.]\n",
            "Episode 19: [10.]\n",
            "Episode 20: [13.]\n",
            "Episode 21: [23.]\n",
            "Episode 22: [17.]\n",
            "Episode 23: [20.]\n",
            "Episode 24: [25.]\n",
            "Episode 25: [12.]\n",
            "Episode 26: [29.]\n",
            "Episode 27: [9.]\n",
            "Episode 28: [11.]\n",
            "Episode 29: [16.]\n",
            "Episode 30: [32.]\n"
          ]
        }
      ],
      "source": [
        "# Modify the play_episodes method to play randomly for a given number of episodes\n",
        "def play_episodes(self, num_episodes=20):\n",
        "    for episode in range(num_episodes):\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = self.env.action_space.sample()  # Take a random action\n",
        "            state, reward, done, _ = self.env.step(action)\n",
        "            self.env.render()  # Optional: render the game\n",
        "\n",
        "# Play the CartPole game randomly for 30 episodes\n",
        "CartPole_agent.play_episodes(num_episodes=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgBhDR1BAQuF",
        "outputId": "416ccebd-ae83-450b-8c22-a12866901002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=20000, episode_reward=9.40 +/- 0.49\n",
            "Episode length: 9.40 +/- 0.49\n",
            "New best mean reward!\n",
            "Eval num_timesteps=40000, episode_reward=119.80 +/- 33.84\n",
            "Episode length: 119.80 +/- 33.84\n",
            "New best mean reward!\n",
            "Eval num_timesteps=60000, episode_reward=170.80 +/- 42.05\n",
            "Episode length: 170.80 +/- 42.05\n",
            "New best mean reward!\n",
            "Eval num_timesteps=80000, episode_reward=90.20 +/- 26.59\n",
            "Episode length: 90.20 +/- 26.59\n",
            "Eval num_timesteps=100000, episode_reward=226.60 +/- 30.47\n",
            "Episode length: 226.60 +/- 30.47\n",
            "New best mean reward!\n",
            "Eval num_timesteps=120000, episode_reward=149.20 +/- 6.82\n",
            "Episode length: 149.20 +/- 6.82\n",
            "Eval num_timesteps=140000, episode_reward=143.80 +/- 8.52\n",
            "Episode length: 143.80 +/- 8.52\n",
            "Eval num_timesteps=160000, episode_reward=9.60 +/- 0.49\n",
            "Episode length: 9.60 +/- 0.49\n",
            "Eval num_timesteps=180000, episode_reward=9.40 +/- 0.49\n",
            "Episode length: 9.40 +/- 0.49\n",
            "Eval num_timesteps=200000, episode_reward=103.40 +/- 26.09\n",
            "Episode length: 103.40 +/- 26.09\n"
          ]
        }
      ],
      "source": [
        "# Modify the train method to include custom early stopping criteria\n",
        "def train(self, time_steps=200000, stop_value=500):\n",
        "    best_performance = 0\n",
        "    for step in range(time_steps):\n",
        "        # Training logic here\n",
        "        if step % 10000 == 0:  # Log every 10,000 steps\n",
        "            print(f\"Time Step: {step}, Best Performance so far: {best_performance}\")\n",
        "\n",
        "        # Custom early stopping condition\n",
        "        if self.current_performance > best_performance:\n",
        "            best_performance = self.current_performance\n",
        "\n",
        "        if self.current_performance >= stop_value:\n",
        "            print(f\"Stopping early at step {step}, reached stop value with performance of {self.current_performance}\")\n",
        "            break\n",
        "\n",
        "# Test out the agent with the CartPole game\n",
        "CartPole_agent.train(time_steps=200000, stop_value=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd9sDTVGAQxp",
        "outputId": "ae37f057-9641-4192-90ff-3f1aa2bf5a7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: [63.]\n",
            "Episode 2: [136.]\n",
            "Episode 3: [78.]\n",
            "Episode 4: [82.]\n",
            "Episode 5: [144.]\n",
            "Episode 6: [104.]\n",
            "Episode 7: [133.]\n",
            "Episode 8: [98.]\n",
            "Episode 9: [78.]\n",
            "Episode 10: [67.]\n",
            "Moviepy - Building video /content/Saved_Models/DQN_CartPole_Model/CartPole_Agent_play/CartPole-agent-play-episode-0.mp4.\n",
            "Moviepy - Writing video /content/Saved_Models/DQN_CartPole_Model/CartPole_Agent_play/CartPole-agent-play-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                               "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/Saved_Models/DQN_CartPole_Model/CartPole_Agent_play/CartPole-agent-play-episode-0.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "# Modify the play_episodes method to support different play types\n",
        "def play_episodes(self, num_episodes=10, play_type=\"predict\"):\n",
        "    for episode in range(num_episodes):\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            if play_type == \"predict\":\n",
        "                action = self.predict(state)  # Predict action using the agent's model\n",
        "            elif play_type == \"random\":\n",
        "                action = self.env.action_space.sample()  # Take a random action\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown play_type: {play_type}\")\n",
        "\n",
        "            state, reward, done, _ = self.env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "        print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
        "\n",
        "# Test out the agent with the CartPole game for 10 episodes using the \"predict\" play type\n",
        "CartPole_agent.play_episodes(num_episodes=10, play_type=\"predict\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQOB73FM1Yru"
      },
      "outputs": [],
      "source": [
        "#close the environment\n",
        "CartPole_agent.close_env()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp4V6nG4GEGb"
      },
      "source": [
        "# DQNAgent for SpaceInvaders and Pac-Man"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxHI8VtwAQ0Q"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, name=None, env_name=None, eval_freq=20000, buffer_size=1000):\n",
        "        self.name = name\n",
        "        self.env_name = env_name\n",
        "        self.eval_freq = eval_freq\n",
        "        self.buffer_size = buffer_size\n",
        "        self.log_path = os.path.join('Training/DQN_' + self.name + '_Log')\n",
        "        self.save_path = os.path.join('Saved_Models/DQN_' + self.name + '_Model')\n",
        "        self.env = self.make_environment()\n",
        "        self.model = self._build_dqn()\n",
        "\n",
        "    def make_environment(self):\n",
        "        env = gym.make(self.env_name, render_mode=\"rgb_array\")\n",
        "        env = ResizeObservation(env, 84)\n",
        "        return env\n",
        "\n",
        "    def _build_dqn(self):\n",
        "        model = DQN(CnnPolicy, self.env, verbose=0, tensorboard_log=self.log_path, buffer_size=self.buffer_size)\n",
        "        return model\n",
        "\n",
        "    def _play_one_episode(self):\n",
        "        obs, _ = self.env.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "\n",
        "        while not done:\n",
        "            action = self.env.action_space.sample()\n",
        "            obs, reward, done, *info = self.env.step(action)\n",
        "            score += reward\n",
        "\n",
        "        return score\n",
        "\n",
        "    def play_episodes(self, num_episodes=10, play_type=\"random\"):\n",
        "        if play_type == \"random\":\n",
        "            print(f\"Playing the {self.name} game randomly for {num_episodes} episodes\")\n",
        "            scores = [self._play_one_episode() for _ in range(num_episodes)]\n",
        "            for episode, score in enumerate(scores, 1):\n",
        "                print(f\"Episode {episode}: {score}\")\n",
        "\n",
        "        if play_type == \"predict\":\n",
        "            episode_rewards = []\n",
        "            frames = []\n",
        "\n",
        "            for episode in range(num_episodes):\n",
        "                obs, _ = self.env.reset()\n",
        "                done = False\n",
        "                score = 0\n",
        "\n",
        "                while not done:\n",
        "                    action, _ = self.model.predict(obs)\n",
        "                    obs, reward, done, *info = self.env.step(action)\n",
        "                    score += reward\n",
        "                    frame = Image.fromarray(self.env.render())\n",
        "                    frame = np.array(frame)\n",
        "                    frames.append(frame)\n",
        "\n",
        "                episode_rewards.append(score)\n",
        "\n",
        "                print(f\"Episode {episode+1}: {score}\")\n",
        "\n",
        "            video_path = os.path.join(self.save_path, self.name + \"_Agent_play\")\n",
        "\n",
        "            save_video(frames, video_path, fps=30, name_prefix=f\"{self.name}-agent-play\")\n",
        "\n",
        "    def train(self, time_steps=None, stop_value=None):\n",
        "        stop_callback = StopTrainingOnRewardThreshold(reward_threshold=stop_value, verbose=0)\n",
        "        eval_callback = EvalCallback(self.env, callback_on_new_best=stop_callback, eval_freq=self.eval_freq, best_model_save_path=self.save_path)\n",
        "        self.model.learn(total_timesteps=time_steps, callback=eval_callback)\n",
        "\n",
        "    def evaluate_policy(self, episodes=None):\n",
        "        mean_reward, reward_std = evaluate_policy(self.model, self.env, n_eval_episodes=episodes)\n",
        "        print(f\"Mean reward over {episodes} episodes is {mean_reward} with a standard deviation of {reward_std}\")\n",
        "\n",
        "    def load_best_model(self):\n",
        "        best_model = DQN.load(self.save_path + \"/best_model\")\n",
        "        return best_model\n",
        "\n",
        "    def save_model(self):\n",
        "        return self.model.save(self.save_path)\n",
        "\n",
        "    def close_env(self):\n",
        "        self.env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO63ctPA0v5g"
      },
      "source": [
        "# SpaceInvaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrpuXIvoHVZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dfe65e5-0de2-4dbb-f4a6-8e06b2ea2380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing the SpaceInvaders agent...\n",
            "Agent object: <__main__.DQNAgent object at 0x7b73e22d6410>\n",
            "Attributes: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_build_dqn', '_play_one_episode', 'buffer_size', 'close_env', 'env', 'env_name', 'eval_freq', 'evaluate_policy', 'load_best_model', 'log_path', 'make_environment', 'model', 'name', 'play_episodes', 'save_model', 'save_path', 'train']\n",
            "SpaceInvaders agent initialized with environment: SpaceInvaders-v4\n"
          ]
        }
      ],
      "source": [
        "print(\"Initializing the SpaceInvaders agent...\")\n",
        "try:\n",
        "    SpaceInvaders_agent = DQNAgent(name=\"SpaceInvaders\", env_name=\"SpaceInvaders-v4\")\n",
        "    print(\"Agent object:\", SpaceInvaders_agent)\n",
        "    print(\"Attributes:\", dir(SpaceInvaders_agent))\n",
        "\n",
        "    # Check if env_name exists\n",
        "    if hasattr(SpaceInvaders_agent, 'env_name'):\n",
        "        print(\"SpaceInvaders agent initialized with environment:\", SpaceInvaders_agent.env_name)\n",
        "    else:\n",
        "        print(\"env_name attribute is not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during initialization: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX3848uYH1Wv",
        "outputId": "2f1fab30-4166-4e6c-9835-9ae9395843b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Playing the SpaceInvaders game randomly for 30 episodes\n",
            "Episode 1: 110.0\n",
            "Episode 2: 135.0\n",
            "Episode 3: 210.0\n",
            "Episode 4: 5.0\n",
            "Episode 5: 285.0\n",
            "Episode 6: 75.0\n",
            "Episode 7: 210.0\n",
            "Episode 8: 315.0\n",
            "Episode 9: 235.0\n",
            "Episode 10: 515.0\n",
            "Episode 11: 255.0\n",
            "Episode 12: 20.0\n",
            "Episode 13: 385.0\n",
            "Episode 14: 100.0\n",
            "Episode 15: 145.0\n",
            "Episode 16: 130.0\n",
            "Episode 17: 125.0\n",
            "Episode 18: 120.0\n",
            "Episode 19: 45.0\n",
            "Episode 20: 15.0\n",
            "Episode 21: 180.0\n",
            "Episode 22: 330.0\n",
            "Episode 23: 80.0\n",
            "Episode 24: 120.0\n",
            "Episode 25: 35.0\n",
            "Episode 26: 65.0\n",
            "Episode 27: 415.0\n",
            "Episode 28: 105.0\n",
            "Episode 29: 210.0\n",
            "Episode 30: 80.0\n"
          ]
        }
      ],
      "source": [
        "# test out the agent with the space invaders game\n",
        "SpaceInvaders_agent.play_episodes(num_episodes=30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Import correct Atari environment depending on whether gym or gymnasium is used\n",
        "import gymnasium as gym  # if using gymnasium (remove this if using gym)\n",
        "\n",
        "# Wrap the environment in a VecEnv\n",
        "env = DummyVecEnv([lambda: gym.make(\"ALE/SpaceInvaders-v5\")])  # For Atari environments in Gymnasium\n",
        "\n",
        "# Initialize the DQN agent\n",
        "model = DQN(\"CnnPolicy\", env, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=10000)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"dqn_space_invaders\")\n",
        "\n",
        "# To close the environment\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZXL-boOSvpc",
        "outputId": "1db5a940-43a3-4328-97a6-6a4094ec77e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env in a VecTransposeImage.\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 203      |\n",
            "|    time_elapsed     | 12       |\n",
            "|    total_timesteps  | 2571     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.000683 |\n",
            "|    n_updates        | 617      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 197      |\n",
            "|    time_elapsed     | 24       |\n",
            "|    total_timesteps  | 4862     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.452    |\n",
            "|    n_updates        | 1190     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 194      |\n",
            "|    time_elapsed     | 33       |\n",
            "|    total_timesteps  | 6480     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.607    |\n",
            "|    n_updates        | 1594     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16       |\n",
            "|    fps              | 193      |\n",
            "|    time_elapsed     | 46       |\n",
            "|    total_timesteps  | 9050     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.294    |\n",
            "|    n_updates        | 2237     |\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TptTs3RzbZp"
      },
      "source": [
        "# Pacman"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bi599sNy-Sc"
      },
      "outputs": [],
      "source": [
        "#initialize the agent and create the environment\n",
        "Pacman_agent_agent = DQNAgent(name=\"Pacman\", env_name=\"MsPacman-v4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN0bOZrEy-bd",
        "outputId": "97063435-6b9b-4336-cf95-1d7155190640",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Playing the Pacman game randomly for 30 episodes\n",
            "Episode 1: 240.0\n",
            "Episode 2: 130.0\n",
            "Episode 3: 250.0\n",
            "Episode 4: 220.0\n",
            "Episode 5: 250.0\n",
            "Episode 6: 230.0\n",
            "Episode 7: 180.0\n",
            "Episode 8: 240.0\n",
            "Episode 9: 190.0\n",
            "Episode 10: 180.0\n",
            "Episode 11: 190.0\n",
            "Episode 12: 250.0\n",
            "Episode 13: 230.0\n",
            "Episode 14: 120.0\n",
            "Episode 15: 120.0\n",
            "Episode 16: 120.0\n",
            "Episode 17: 230.0\n",
            "Episode 18: 210.0\n",
            "Episode 19: 210.0\n",
            "Episode 20: 190.0\n",
            "Episode 21: 280.0\n",
            "Episode 22: 280.0\n",
            "Episode 23: 250.0\n",
            "Episode 24: 260.0\n",
            "Episode 25: 320.0\n",
            "Episode 26: 160.0\n",
            "Episode 27: 210.0\n",
            "Episode 28: 250.0\n",
            "Episode 29: 160.0\n",
            "Episode 30: 210.0\n"
          ]
        }
      ],
      "source": [
        "#Play the pacman game randomly for 30 episodes\n",
        "Pacman_agent_agent.play_episodes(num_episodes=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOo_tlGiy-oh",
        "outputId": "8836cb64-f4bb-46fd-c3de-0926d80310ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# Create the Pacman environment (or whatever environment you are using)\n",
        "env = gym.make(\"MsPacman-v0\")\n",
        "\n",
        "# Initialize the DQN agent\n",
        "model = DQN(\"CnnPolicy\", env, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEwXh6nO1BYo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b027da-ca5d-4cd9-8dab-31e04f3935b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to pacman_dqn_model.h5\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, name, env_name):\n",
        "        # Initialize the environment\n",
        "        self.env = gym.make(env_name)\n",
        "        self.state_size = self.env.observation_space.shape[0]  # Define state_size from environment\n",
        "        self.action_size = self.env.action_space.n  # Number of actions\n",
        "        self.name = name\n",
        "        self.model = self.build_model()  # Build the neural network model\n",
        "\n",
        "    def build_model(self):\n",
        "        # Build a simple neural network using Keras for the DQN model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))  # Output layer for Q-values\n",
        "        model.compile(loss='mse', optimizer='adam')\n",
        "        return model\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        # Use Keras' save function to save the model\n",
        "        self.model.save(filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "# Example usage:\n",
        "Pacman_agent_agent = DQNAgent(name=\"Pacman\", env_name=\"MsPacman-v0\")  # Adjust the environment name\n",
        "Pacman_agent_agent.save_model(\"pacman_dqn_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8kAQe2PzX2T"
      },
      "outputs": [],
      "source": [
        "def close_env(self):\n",
        "    # Close the environment\n",
        "    self.env.close()\n",
        "    print(f\"Environment for {self.name} has been closed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, name, env_name):\n",
        "        # Initialize the environment\n",
        "        self.env = gym.make(env_name)\n",
        "        self.state_size = np.prod(self.env.observation_space.shape)  # Flatten the state space\n",
        "        self.action_size = self.env.action_space.n  # Action space size\n",
        "        self.name = name\n",
        "        self.model = self.build_model()  # Build the DQN model\n",
        "\n",
        "    def build_model(self):\n",
        "        # Build a simple neural network for the DQN model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))  # Q-values for each action\n",
        "        model.compile(loss='mse', optimizer='adam')\n",
        "        return model\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        # Save the model to a file\n",
        "        self.model.save(filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def close_env(self):\n",
        "        # Close the environment\n",
        "        self.env.close()\n",
        "        print(f\"Environment for {self.name} has been closed.\")\n",
        "\n",
        "\n",
        "# Space Invaders\n",
        "spaceinvaders_agent = DQNAgent(name=\"SpaceInvaders\", env_name=\"SpaceInvaders-v0\")\n",
        "spaceinvaders_agent.save_model(\"SpaceInvaders_final_model.h5\")  # Save final model\n",
        "spaceinvaders_agent.close_env()\n",
        "\n",
        "# CartPole\n",
        "cartpole_agent = DQNAgent(name=\"CartPole\", env_name=\"CartPole-v0\")\n",
        "cartpole_agent.save_model(\"CartPole_final_model.h5\")  # Save final model\n",
        "cartpole_agent.close_env()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkOv7OmQxAih",
        "outputId": "acf98258-ff83-4a7b-ef39-b8228ea80616"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to SpaceInvaders_final_model.h5\n",
            "Environment for SpaceInvaders has been closed.\n",
            "Model saved to CartPole_final_model.h5\n",
            "Environment for CartPole has been closed.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}